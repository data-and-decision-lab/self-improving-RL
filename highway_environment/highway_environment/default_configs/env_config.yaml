config:
    lanes_count:              1
    initial_lane_id:          0                 # [change this to start agent from spesific lane id]
    vehicles_count:           1                 # [number of IDM vehicles (not controlled vehicles)]
    controlled_vehicles:      1                 # [number of controlled vehicles]
    episode_length:           250               # [steps]
    duration:                 60                # [s]
    simulation_frequency:     15                # [hz]
    policy_frequency:         5                 # [hz]
    
    collision_reward:         -10.0             # [reward received when colliding with a vehicle]
    offroad_terminal:         true              # [create termination condition for road deviation]
    offroad_reward:           -5.0              # [punishment received when agent gets out of road]
    speed_range:              [ 10.0, 40.0 ]    # [m/s]
    speed_std:                2.5
    min_tgap:                 0.5               # [minimum time gap limit seconds for reward configuration]
    max_tgap:                 5.0               # [maximum time gap limit seconds for reward configuration]
    tgap_mean:                1.2
    tgap_std:                 0.25
    min_ttc:                  6.0               # [ttc reward is given when ttc < min_ttc]
    max_ttc:                  99.0              # [ttc is normalized by deviding ttc by max_ttc]
    rew_tgap_range:           [ 0.8, 2.0 ]      # [negative reward is given then time-gap is outside this range]
    rew_tgap_coef:            0.6               # [time-gap punishment multiplier coefficient]
    rew_ttc_coef:             1.2               # [ttc reward is inversely multiplied with rew_ttc_coef]
    rew_u_range:              [ -0.50, 0.5 ]    # [throttle-brake action reward for being between this range]
    rew_u_coef:               0.3               # [positive reward coeficient for throttle-brake in rew_u_range]
    rew_speed_coef:           4.0               # [velocity multiplier for reward function]
    rew_jerk_lim:             3.0               # [negative reward is given when abs(ego jerk) > rew_jerk_lim]
    rew_jerk_coef:            -0.05             # [jerk punishment multiplier when jerk is higher than rew_jerk_lim]
    rew_steer_coef:           -2.0              # [steering action punishment coefficient]
    
    screen_width:             600               # [px]
    screen_height:            150               # [px]
    centering_position:       [ 0.3, 0.5 ]
    scaling:                  5.5
    
    other_vehicles_type:      "highway_env.vehicle.behavior.IDMVehicle"
    idm_max_accel:            4.0               # [maximum acceleration (m/s^2)]
    idm_comfort_acc_max:      2.0               # [desired maximum acceleration (m/s^2)]
    idm_comfort_acc_min:      -2.0              # [desired maximum deceleration (m/s^2)]
    idm_distance_wanted:      3.0               # [desired jam distance to the front vehicle (m)]
    idm_time_wanted:          1.2               # [desired time gap to the front vehicle (s)]
    idm_delta:                4.0               # [acceleration exponent]
    show_trajectories:        false
    manual_control:           false             # [true: manual control is enabled (make sure that real_time_rendering=true)]
    
    offscreen_rendering:      false             # [instruction allows the drawing to be done on surfaces without handling a screen display]
    rendering_mode:           rgb_array         # ['human', 'rgb_array']
    render_agent:             true
    real_time_rendering:      false             # [true: when manual control or evaluation with rendering is true]
    rendering:                false             # [true: to visualize each episode]
    
    set_manually:             {}                # [search space -> when initial speed and positions are set manually, especially during verifications]
    scenario_config:
        type:                 null              # [custom scenario evaluation class -> experiments.utils.scenarios.py (works with self_improvement.yaml config)]
        file_name:            null              # [csv file name of verification results (works with self_improvement.yaml config)]
    
    observation:
        type:                 Kinematics
    
    action:
        type:                 ContinuousAction
        acceleration_range:   [ -4.0, 4.0 ]     # [m/s^2]
        steering_range:       [ -1.0, 1.0 ]     # [radian]
        speed_range:          [ 10.0, 40.0 ]    # [m/s]
        longitudinal:         true              # [whether to enable throttle control]
        lateral:              false             # [whether to enable steering control]
        dynamical:            false             # [whether to simulate dynamics (i.e. friction) rather than kinematics]
        clip:                 true              # [clip action to the defined range]
