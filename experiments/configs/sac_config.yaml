_deterministic_loss:                            false
_disable_action_flattening:                     false
_disable_execution_plan_api:                    true
_disable_preprocessor_api:                      false
_fake_gpus:                                     false
_tf_policy_handles_more_than_one_loss:          false
_use_beta_distribution:                         false

actions_in_input_normalized:                    false
always_attach_evaluation_results:               false
batch_mode:                                     truncate_episodes
clip_actions:                                   true
collect_metrics_timeout:                        -1
fake_sampler:                                   false
ignore_worker_failures:                         false

compress_observations:                          false
create_env_on_driver:                           false
custom_resources_per_worker:                    {}
disable_env_checking:                           false
eager_max_retraces:                             20
eager_tracing:                                  false
enable_connectors:                              false
enable_tf1_exec_eagerly:                        false

exploration_config:
    type:                                       StochasticSampling
explore:                                        true

extra_python_environs_for_driver:               {}
extra_python_environs_for_worker:               {}
keep_per_episode_custom_metrics:                false

env:                                            "highway_environment:highway-environment-v0"
framework:                                      torch
gamma:                                          0.95
initial_alpha:                                  1.0
learning_starts:                                -1
lr:                                             0.0003
log_level:                                      INFO
log_sys_usage:                                  true
monitor:                                        -1
horizon:                                        250

input:                                          sampler
input_config:                                   {}
input_evaluation:                               -1

local_tf_session_args:
    inter_op_parallelism_threads:               8
    intra_op_parallelism_threads:               8

metrics_episode_collection_timeout_s:           180.0
metrics_num_episodes_for_smoothing:             100
metrics_smoothing_episodes:                     -1
min_iter_time_s:                                -1
min_sample_timesteps_per_iteration:             0
min_sample_timesteps_per_reporting:             -1
min_time_s_per_iteration:                       1
min_time_s_per_reporting:                       -1
min_train_timesteps_per_iteration:              0
min_train_timesteps_per_reporting:              -1

render_env:                                     false
rollout_fragment_length:                        1024
train_batch_size:                               128
replay_batch_size:                              -1
tau:                                            0.005

replay_buffer_config:
    _enable_replay_buffer_api:                  true
    capacity:                                   10000
    learning_starts:                            3072
    prioritized_replay:                         false
    prioritized_replay_alpha:                   0.6
    prioritized_replay_beta:                    0.4
    prioritized_replay_eps:                     1.0e-06
    type:                                       MultiAgentPrioritizedReplayBuffer
    worker_side_prioritization:                 false

model:
    _disable_action_flattening:                 false
    _disable_preprocessor_api:                  false
    _time_major:                                false
    _use_default_native_models:                 false
    
    use_attention:                              false
    attention_dim:                              64
    attention_head_dim:                         32
    attention_init_gru_gate_bias:               2.0
    attention_memory_inference:                 50
    attention_memory_training:                  50
    attention_num_heads:                        1
    attention_num_transformer_units:            1
    attention_position_wise_mlp_dim:            32
    attention_use_n_prev_actions:               0
    attention_use_n_prev_rewards:               0
    
    conv_activation:                            relu
    conv_filters:                               null
    
    custom_action_dist:                         null
    custom_model:                               null
    custom_model_config:                        {}
    custom_preprocessor:                        null
    
    dim:                                        84
    fcnet_activation:                           tanh
    fcnet_hiddens:                              [ 64, 64 ]
    
    framestack:                                 false
    free_log_std:                               false
    grayscale:                                  false

    use_lstm:                                   false
    lstm_cell_size:                             256
    lstm_use_prev_action:                       false
    lstm_use_prev_action_reward:                -1
    lstm_use_prev_reward:                       false
    max_seq_len:                                20
    no_final_linear:                            false
    post_fcnet_activation:                      relu
    post_fcnet_hiddens:                         []
    vf_share_layers:                            false
    zero_mean:                                  true

multiagent:
    count_steps_by:                             env_steps
    observation_fn:                             null
    policies:                                   {}
    policies_to_train:                          null
    policy_map_cache:                           null
    policy_map_capacity:                        100
    policy_mapping_fn:                          null

n_step:                                         1
no_done_at_end:                                 false
normalize_actions:                              true
num_consecutive_worker_failures_tolerance:      100

num_cpus_for_driver:                            1
num_cpus_per_worker:                            1
num_envs_per_worker:                            1
num_workers:                                    3

observation_filter:                             NoFilter

optimizer:                                      {}
optimization:
    actor_learning_rate:                        0.0005
    critic_learning_rate:                       0.005
    entropy_learning_rate:                      0.0005

output_config:                                  {}
output_max_file_size:                           67108864
output_compress_columns:
    - obs
    - new_obs

placement_strategy:                             PACK
policy_model_config:
    custom_model:                               null
    custom_model_config:                        {}
    fcnet_activation:                           tanh
    fcnet_hiddens:                              [ 64, 64 ]
    post_fcnet_activation:                      null
    post_fcnet_hiddens:                         []

postprocess_inputs:                             false
preprocessor_pref:                              deepmind

prioritized_replay:                             -1
prioritized_replay_alpha:                       -1
prioritized_replay_beta:                        -1
prioritized_replay_eps:                         -1

q_model_config:
    custom_model:                               null
    custom_model_config:                        {}
    fcnet_activation:                           relu
    fcnet_hiddens:                              [ 64, 64 ]
    post_fcnet_activation:                      null
    post_fcnet_hiddens:                         []

recreate_failed_workers:                        false
remote_env_batch_wait_ms:                       0
remote_worker_envs:                             false

restart_failed_sub_environments:                false
sample_async:                                   false
shuffle_buffer_size:                            0
simple_optimizer:                               -1
soft_horizon:                                   false
store_buffer_in_checkpoints:                    false
sync_filters_on_rollout_workers_timeout_s:      60.0
synchronize_filters:                            true
target_entropy:                                 auto
target_network_update_freq:                     0

tf_session_args:
    allow_soft_placement:                       true
    device_count:
        CPU:                                    1
    gpu_options:
        allow_growth:                           true
    inter_op_parallelism_threads:               2
    intra_op_parallelism_threads:               2
    log_device_placement:                       false

timesteps_per_iteration:                        -1
twin_q:                                         true
use_state_preprocessor:                         -1
validate_workers_after_construction:            true
worker_side_prioritization:                     -1

evaluation_config:
    off_policy_estimation_methods:
        - ray.rllib.offline.estimators.importance_sampling.ImportanceSampling
        - ray.rllib.offline.estimators.weighted_importance_sampling.WeightedImportanceSampling
evaluation_duration:                            10
evaluation_duration_unit:                       episodes
evaluation_num_workers:                         0
evaluation_parallel_to_training:                false

env_config:                                     {}