# config for -> python ./experiments/utils/reward_tuning.py

experiment_name:        reward_tuning_grid_search

# [path to folder including real driving trajectories inside experiments folder (currently private)]
dataset_folder_path:    "dataset/eatron_dataset"

resume:                 False
num_samples:            1       # [number of time grid search will be repeated (default: 1)]
num_tune_scenarios:     100     # [number of how many different ACC scenarios going to be used from real driving trajectories]

search_space:
    velocity_multiplier: 
        min:            1.0
        max:            5.0
        steps:          9
    
    action_coefficient:
        min:            0.1
        max:            1.0
        steps:          10
    
    timegap_coefficient:
        min:            0.1
        max:            2.0
        steps:          20
    
    ttc_multiplier:
        min:            0.1
        max:            2.0
        steps:          20

ray_tune_resources:
    cpu:                2       # [number of CPUs to allocate to the trial -> total cpu allocation = cpu * (1/gpu)]
    gpu:                0.2     # [number of GPUs to allocate to the trial]

metric:                 reward  # [metric to optimize (should be in tune.report() arguments)]
mode:                   max     # [objective to apply ([maximize -> max] or [minimize -> min])]